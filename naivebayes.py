
"""
    The aim of this module is to implement the Naive Bayes classification algorithm
    for text classification.
    
    This algorithm will include:
    a feature extractor:
        this builds the contingency table used by the classifier
        it will take an array of features and their classes
    a classifier:
        this will use the contingency table generated by the feature extractor to
        predict the class of a new feature
"""

import sys
import os
import nltk


class Counter(dict):
    def __missing__(self, key):
        return 1


class NaiveBayes(object):
    def __init__(self, *args, **kwargs):
        self._features = None
        self._classes = None
        self._total_words = None

    def __str__(self):
        return "Cleverchuk's Simple Naive Bayes Classifier"

    def getFeature(self):
        return self._features
    
    def getClasses(self):
        return self._classes

    def getTotalWords(self):
        return self._total_words

    def listWords(self, text):
        """
            utility method
        """
        words = []        
        for w in map(str.lower, nltk.word_tokenize(text)): # n
            if len(w) > 2: # only consider words of 3 or more as features
                words.append(w)

        return words

    def train(self, feature_arr, class_arr):
        """
            param: feature_arr array of features only strings valid features
            param: class_arr array of classes for the features in feature_arr

            this method is used to created a virtual contingency table used to 
            calculate the probabilities for the Bayes theorem
        """
        self._features = {}  # a dictionary representing the contingency table
        self._classes = Counter()  # a dictionary for keeping count of class
        self._total_words = 0

        # validate data size
        if len(feature_arr) != len(class_arr):
            raise Exception(
                "length of feature_arr must equal length of class_arr")
        
        # validate data type
        if not all(type(item) is str for item in feature_arr) and \
            not all(type(item) is str for item in class_arr):
            raise ValueError("must be a list of strings")
        
        if not type(feature_arr) is list and not type(class_arr) is list:
            raise TypeError("feature_arr and class_arr must be lists")


        for i in range(len(feature_arr)): # n^3
            words = self.listWords(feature_arr[i])

            # frequency of class
            if class_arr[i] not in self._classes:
                self._classes[class_arr[i]] = 1
            else:
                self._classes[class_arr[i]] += 1

            # aggregate word occurrence in each class
            for word in words:
                if word not in self._features:
                    counter = {}
                    for c in class_arr:
                        counter[c] = 0

                    self._features[word] = counter
                    self._total_words += 1  # aggregates the total observations in the table

                self._features[word][class_arr[i]] += 1

        return (self._features, self._classes, self._total_words)

    def classify(self, sentence):
        """
            param: sentence sentence to classify
            this method classifies a sentence
        """
        words = self.listWords(sentence)
        _class = None
        prob_f = 0

        for c in iter(self._classes.keys()): # n^2
            # calculate P(C)
            prob_c = self._classes[c]/float(sum(self._classes.values()))  # P(C)
            prob_total = prob_c

            for w in words:
                if w in self._features:
                    
                    prob_w = sum(self._features[w].values()) / float(self._total_words) # P(W)
                    prob_wnc = self._features[w][c] / float(self._total_words) # P(W and C)
                    prob_cond = prob_wnc / prob_w # P(C|W)  
                    prob = prob_cond*prob_w/prob_c  # P(W|C)
                    prob_total *= prob

                    if prob_f < prob_total:
                        prob_f = prob
                        _class = c

        return (_class, prob_f)



